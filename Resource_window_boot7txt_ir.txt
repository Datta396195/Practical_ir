Web Crawling and Indexing
1. Develop a web crawler to fetch and index web pages
2. Handle challenges such as robots.txt, dynamic content and crawling delays


import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin
from urllib.robotparser import RobotFileParser

def fetch_page(url):
    try:
        response = requests.get(url)
        return response.text if response.status_code == 200 else None
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None

def extract_links(page_content, base_url):
    soup = BeautifulSoup(page_content, "html.parser")
    return [urljoin(base_url, link["href"]) for link in soup.find_all("a", href=True)]

def crawl(start_url, max_pages=10, delay=1):
    visited_urls, pages_to_visit = set(), [start_url]
    rp = RobotFileParser()
    rp.set_url(urljoin(start_url, "/robots.txt"))
    rp.read()

    while pages_to_visit and len(visited_urls) < max_pages:
        url = pages_to_visit.pop(0)
        if url in visited_urls or not rp.can_fetch("*", url):
            continue
        print(f"Crawling: {url}")
        page_content = fetch_page(url)
        if not page_content:
            continue
        visited_urls.add(url)
        links = extract_links(page_content, url)
        pages_to_visit.extend(links)
        time.sleep(delay)
    print("Crawling has been completed!")

if __name__ == "__main__":
    start_url = "https://mailchimp.com/features/website-builder/"
    crawl(start_url)
